{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jax dedicated libraries\n",
    "from flax import nnx\n",
    "import optax\n",
    "import jax\n",
    "import jax.numpy as jnp # From this point on, there should not be numpy anymore but only jax.numpy\n",
    "import jax.scipy as jsp\n",
    "import orbax.checkpoint as ocp  # Checkpointing library\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Module functions\n",
    "import ximinf.nn_train as nnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from local path\n",
    "data = jnp.load(\"../data/simulations.npy\")\n",
    "\n",
    "N = jnp.shape(data)[0]\n",
    "M = (jnp.shape(data)[1]-3)/4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the different network layers\n",
    "Nsize_p = 2*M\n",
    "Nsize_r = 20*M\n",
    "phi_batch = 1\n",
    "\n",
    "model = nnt.DeepSetClassifier(0.05, Nsize_p, Nsize_r, phi_batch, rngs=nnx.Rngs(0))\n",
    "\n",
    "# Visualize the model structure\n",
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise metrics history\n",
    "metrics_history = {'train_loss': [], 'train_accuracy': [], 'test_loss': [], 'test_accuracy': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate schedule\n",
    "learning_rate_schedule = optax.exponential_decay(\n",
    "    init_value=3e-4,\n",
    "    transition_steps=1000,  # Decay every 1000 forward passes\n",
    "    decay_rate=0.9,\n",
    ")\n",
    "\n",
    "momentum = 0.9 # Necessary for the Adam optimiser\n",
    "\n",
    "# Initialize optimiser with the adaptive learning rate\n",
    "optimizer = nnx.Optimizer(model, optax.adamw(learning_rate_schedule, momentum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping parameters\n",
    "patience = 20 # Number of epochs to wait for improvement\n",
    "epochs = 1000 # Maximum number of epochs\n",
    "\n",
    "# batch_size = N//200 # Number of samples per batch\n",
    "batch_size = 1000\n",
    "\n",
    "# Initialise stopping criteria\n",
    "best_train_loss = jnp.inf\n",
    "best_test_loss = jnp.inf\n",
    "strikes = 0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle the training data using JAX.\n",
    "    key, subkey = jax.random.split(key)\n",
    "    perm = jax.random.permutation(subkey, len(train_data_gpu))\n",
    "    train_data_gpu = train_data_gpu[perm]\n",
    "    train_labels_gpu = train_labels_gpu[perm]\n",
    "    del perm\n",
    "    \n",
    "    epoch_train_loss = 0\n",
    "    epoch_train_accuracy = 0\n",
    "    \n",
    "    for i in range(0, len(train_data_gpu), batch_size):\n",
    "        # Get the current batch of data and labels\n",
    "        batch_data = train_data_gpu[i:i+batch_size]\n",
    "        batch_labels = train_labels_gpu[i:i+batch_size]\n",
    "        \n",
    "        # Perform a training step\n",
    "        loss, _ = nnt.loss_fn(model, (batch_data, batch_labels))\n",
    "        accuracy = nnt.accuracy_fn(model, (batch_data, batch_labels))\n",
    "        epoch_train_loss += loss\n",
    "        epoch_train_accuracy += accuracy\n",
    "        nnt.train_step(model, optimizer, (batch_data, batch_labels))\n",
    "    \n",
    "    # Log the training metrics.\n",
    "    current_train_loss = epoch_train_loss / (len(train_data_gpu) / batch_size)\n",
    "    metrics_history['train_loss'].append(current_train_loss)\n",
    "    metrics_history['train_accuracy'].append(epoch_train_accuracy/(len(train_data_gpu) / batch_size))\n",
    "\n",
    "    epoch_test_loss = 0\n",
    "    epoch_test_accuracy = 0\n",
    "\n",
    "    # Compute the metrics on the test set using the same batching as training\n",
    "    for i in range(0, len(test_data_gpu), batch_size):\n",
    "        batch_data = test_data_gpu[i:i+batch_size]\n",
    "        batch_labels = test_labels_gpu[i:i+batch_size]\n",
    "\n",
    "        loss, _ = nnt.loss_fn(model, (batch_data, batch_labels))\n",
    "        accuracy = nnt.accuracy_fn(model, (batch_data, batch_labels))\n",
    "        epoch_test_loss += loss\n",
    "        epoch_test_accuracy += accuracy\n",
    "\n",
    "    # Log the test metrics.\n",
    "    current_test_loss = epoch_test_loss / (len(test_data_gpu) / batch_size)\n",
    "    metrics_history['test_loss'].append(current_test_loss)\n",
    "    metrics_history['test_accuracy'].append(epoch_test_accuracy/ (len(test_data_gpu) / batch_size))\n",
    "    \n",
    "    # Early Stopping Check\n",
    "    if current_test_loss < best_test_loss:\n",
    "        best_test_loss = current_test_loss  # Update best test loss\n",
    "        strikes = 0\n",
    "    elif current_train_loss >= best_train_loss:\n",
    "        strikes = 0\n",
    "    elif current_test_loss > best_test_loss and current_train_loss < best_train_loss:\n",
    "        strikes +=1\n",
    "    elif current_train_loss < best_train_loss:\n",
    "        best_train_loss = current_train_loss # Update best train loss\n",
    "\n",
    "    if strikes >= patience:\n",
    "        print(f\"\\n Early stopping at epoch {epoch+1} due to {patience} consecutive increases in loss gap \\n\")\n",
    "        break\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        #Plot loss and accuracy in subplots\n",
    "        clear_output(wait=True) # Clear the output to avoid cluttering\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        ax1.set_title(f'Loss for M:{M} and N:{N}')\n",
    "        for dataset in ('train', 'test'):\n",
    "            ax1.plot(metrics_history[f'{dataset}_loss'], label=f'{dataset}_loss')\n",
    "            ax1.legend()\n",
    "            ax1.set_yscale(\"log\")\n",
    "    \n",
    "        ax2.set_title('Accuracy')\n",
    "        for dataset in ('train', 'test'):\n",
    "            ax2.plot(metrics_history[f'{dataset}_accuracy'], label=f'{dataset}_accuracy')\n",
    "            ax2.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # disable dropout, etc.\n",
    "\n",
    "batch_size = 128  # tune this to fit your RAM; lower â†’ safer\n",
    "\n",
    "# Accumulators\n",
    "all_logits = []\n",
    "all_labels = []\n",
    "\n",
    "# Loop over your test set in-place\n",
    "num_samples = test_data_gpu.shape[0]\n",
    "for i in range(0, num_samples, batch_size):\n",
    "    xb = test_data_gpu[i : i + batch_size]\n",
    "    yb = test_labels_gpu[i : i + batch_size]\n",
    "\n",
    "    # Get logits for this mini-batch\n",
    "    logits = nnt.pred_step(model, xb)\n",
    "    all_logits.append(logits)\n",
    "\n",
    "    # Store the corresponding true labels\n",
    "    all_labels.append(yb > 0.5)\n",
    "\n",
    "# Merge everything back together\n",
    "all_logits = jnp.concatenate(all_logits, axis=0)\n",
    "all_preds  = jsp.special.expit(all_logits) > 0.5\n",
    "all_labels = jnp.concatenate(all_labels, axis=0)\n",
    "\n",
    "# === same metrics computation as before ===\n",
    "TP = jnp.sum((all_preds == 1) & (all_labels == 1))\n",
    "TN = jnp.sum((all_preds == 0) & (all_labels == 0))\n",
    "FP = jnp.sum((all_preds == 1) & (all_labels == 0))\n",
    "FN = jnp.sum((all_preds == 0) & (all_labels == 1))\n",
    "\n",
    "print(f\"True positives : {TP}\")\n",
    "print(f\"True negatives : {TN}\")\n",
    "print(f\"False positives: {FP}\")\n",
    "print(f\"False negatives: {FN}\\n\")\n",
    "\n",
    "accuracy    = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision   = TP / (TP + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print(f\"Accuracy   : {accuracy:.3f}\")\n",
    "print(f\"Precision  : {precision:.3f}\")\n",
    "print(f\"Sensitivity: {sensitivity:.3f}\")\n",
    "print(f\"Specificity: {specificity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save NN to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = ocp.test_utils.erase_and_create_empty('/Users/atrigui/Documents/Stage/sbi_stage_trigui/saved_model_12_7_V7')\n",
    "\n",
    "# Split the model into GraphDef (structure) and State (parameters + buffers)\n",
    "_, rng_key, rng_count, state = nnx.split(model, nnx.RngKey, nnx.RngCount, ...)\n",
    "\n",
    "# Display for debugging (optional)\n",
    "nnx.display(state)\n",
    "\n",
    "# Initialize the checkpointer\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "\n",
    "# Save State (parameters & non-trainable variables)\n",
    "checkpointer.save(ckpt_dir / 'state', state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sky",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
