{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jax dedicated libraries\n",
    "from flax import nnx\n",
    "import optax\n",
    "import jax\n",
    "import jax.numpy as jnp # From this point on, there should not be numpy anymore but only jax.numpy\n",
    "import jax.scipy as jsp\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import corner\n",
    "import numpy as np\n",
    "\n",
    "# Module functions\n",
    "import ximinf.nn_train as nntr\n",
    "import ximinf.nn_test as nnte\n",
    "import ximinf.generate_sim as gsim\n",
    "\n",
    "# Other\n",
    "import h5py\n",
    "import gc\n",
    "\n",
    "from absl import logging\n",
    "\n",
    "logging.set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set device type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try GPU backends in priority order\n",
    "gpu = None\n",
    "for backend in (\"METAL\", \"cuda\", \"gpu\"):\n",
    "    try:\n",
    "        devs = jax.devices(backend)\n",
    "    except RuntimeError:\n",
    "        continue\n",
    "    if devs:\n",
    "        gpu = devs[0]\n",
    "        break\n",
    "\n",
    "# Fallback\n",
    "cpu = jax.devices(\"cpu\")[0]\n",
    "\n",
    "# Use GPU if found\n",
    "if gpu=='cuda':\n",
    "    nntr.print_gpu_memory()\n",
    "    device = gpu \n",
    "elif gpu is not None:\n",
    "    device = gpu\n",
    "else:\n",
    "    device = cpu\n",
    "    \n",
    "jax.default_device(device)\n",
    "    \n",
    "backend = jax.default_backend()\n",
    "print(backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the new HDF5 file\n",
    "# file_path = \"./data/SIM/simulations_10000_2_1000_1000_brok_alpha_beta_gamma_sigma_XS_gaussian_priors_cosmo_XL_errors_z.h5\"\n",
    "file_path = \"../data/SIM/simulations_2000_5_1000_1000_brok_alpha_beta_gamma_sigma_XS_gaussian_exponential_priors_errors_z.h5\" #simulations_10000_2_1000_1000_brok_alpha_beta_gamma_sigma_XS_gaussian_priors_cosmo_XL_errors_z\n",
    "\n",
    "with h5py.File(file_path, \"r\") as f:\n",
    "\n",
    "    # Load parameters\n",
    "    params = {k: jnp.array(f[\"params\"][k][:], dtype=jnp.float32) for k in f[\"params\"].keys()}\n",
    "\n",
    "    # Load data\n",
    "    data = {k: jnp.array(f[\"data\"][k][:], dtype=jnp.float32) for k in f[\"data\"].keys()}\n",
    "\n",
    "    # Load priors\n",
    "    priors = {}\n",
    "    for name in f[\"priors\"].keys():\n",
    "        grp = f[\"priors\"][name]\n",
    "        priors[name] = {\n",
    "            \"range\": jnp.array(grp[\"range\"][:], dtype=jnp.float32),\n",
    "            \"type\": grp.attrs[\"type\"]\n",
    "        }\n",
    "\n",
    "# Number of simulations and SNe per simulation\n",
    "N = next(iter(params.values())).shape[0]   # N simulations\n",
    "M = next(iter(data.values())).shape[1]     # M SNe per simulation\n",
    "\n",
    "print(f\"The file contains {N} simulations of size {M}\")\n",
    "print(\"Loaded priors:\", priors)\n",
    "\n",
    "print('Removing cosmology ...')\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "# Take care to not run the correction twice\n",
    "mu_planck18, magobs = nntr.rm_cosmo(data['z'], data['magobs'], package='cosmologix') #, package='cosmologix'\n",
    "print('... done')\n",
    "jax.config.update(\"jax_enable_x64\", False)\n",
    "\n",
    "data['magobs'] = magobs\n",
    "mask = magobs != 0 \n",
    "\n",
    "# Only update 'mabs' if it exists in params\n",
    "if 'mabs' in params:\n",
    "    params['mabs'] = params['mabs'] + 19.3\n",
    "    priors['mabs']['range'] += 19.3\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "print(\"Parameter names:\", list(params.keys()))\n",
    "param_groups = ['mabs', ['alpha_low', 'alpha_high'], 'beta', ['gamma', 'sigma_int']] # Sort them from easiest to hardest to infer , 'Om0'\n",
    "print(param_groups[0])\n",
    "\n",
    "# ----------------------------\n",
    "# Flatten param_groups into a unique ordered list\n",
    "# ----------------------------\n",
    "global_param_names = []\n",
    "for group in param_groups:\n",
    "    group_list = [group] if isinstance(group, str) else group\n",
    "    for p in group_list:\n",
    "        if p not in global_param_names:\n",
    "            global_param_names.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Column names:\", list(data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked version (NaNs)\n",
    "data_filt = {k: jnp.where(mask, v, jnp.nan) for k, v in data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 102  # index for your data slice\n",
    "\n",
    "# Define the color maps\n",
    "cmap1 = LinearSegmentedColormap.from_list(\n",
    "    'custom_red_beige_blue',\n",
    "    ['#1F487E', 'beige', '#A31621']\n",
    ")\n",
    "\n",
    "cmap2 = LinearSegmentedColormap.from_list(\n",
    "    'custom_green_beige_purple',\n",
    "    ['#687444', 'beige', '#5E4983']\n",
    ")\n",
    "\n",
    "cmap3 = LinearSegmentedColormap.from_list(\n",
    "    'custom_blue_beige_orange',\n",
    "    ['#1F487E', 'beige', '#C07835']\n",
    ")\n",
    "\n",
    "# Create figure and horizontal subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4), constrained_layout=True)\n",
    "\n",
    "# First subplot: z vs magobs\n",
    "sc1 = axes[0].scatter(\n",
    "    data_filt['z'][index, :],\n",
    "    data_filt['magobs'][index, :],\n",
    "    c=data_filt['z'][index, :],\n",
    "    cmap=cmap1,\n",
    "    edgecolor='k'\n",
    ")\n",
    "axes[0].set_title('Magnitude vs Redshift', fontsize=14)\n",
    "axes[0].set_xlabel('Redshift (z)', fontsize=12)\n",
    "axes[0].set_ylabel('Observed Magnitude', fontsize=12)\n",
    "cbar1 = plt.colorbar(sc1, ax=axes[0])\n",
    "cbar1.set_label('Redshift z', fontsize=12)\n",
    "\n",
    "# Second subplot: c vs magobs\n",
    "sc2 = axes[1].scatter(\n",
    "    data_filt['c'][index, :],\n",
    "    data_filt['magobs'][index, :],\n",
    "    c=data_filt['c'][index, :],\n",
    "    cmap=cmap2,\n",
    "    edgecolor='k'\n",
    ")\n",
    "axes[1].set_title('Magnitude vs Color', fontsize=14)\n",
    "axes[1].set_xlabel('Color (c)', fontsize=12)\n",
    "axes[1].set_ylabel('Observed Magnitude', fontsize=12)\n",
    "cbar2 = plt.colorbar(sc2, ax=axes[1])\n",
    "cbar2.set_label('Color value', fontsize=12)\n",
    "\n",
    "# Third subplot: x1 vs magobs\n",
    "sc3 = axes[2].scatter(\n",
    "    data_filt['x1'][index, :],\n",
    "    data_filt['magobs'][index, :],\n",
    "    c=data_filt['x1'][index, :],\n",
    "    cmap=cmap3,\n",
    "    edgecolor='k'\n",
    ")\n",
    "axes[2].set_title('Magnitude vs Stretch', fontsize=14)\n",
    "axes[2].set_xlabel('Stretch (x1)', fontsize=12)\n",
    "axes[2].set_ylabel('Observed Magnitude', fontsize=12)\n",
    "cbar3 = plt.colorbar(sc3, ax=axes[2])\n",
    "cbar3.set_label('Stretch x1', fontsize=12)\n",
    "\n",
    "# Construct the title string dynamically\n",
    "title_str = \", \".join(f\"{name} = {params[name][index]:.2f}\" for name in global_param_names)\n",
    "\n",
    "fig.suptitle(\n",
    "    title_str,\n",
    "    fontsize=16\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "gc.collect() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padded version (zeros)\n",
    "data_padded = {k: jnp.where(mask, v, 0) for k, v in data.items()}\n",
    "\n",
    "# select_columns = ['magobs', 'magobs_err', 'c', 'c_err', 'x1','x1_err', 'mass', 'mass_err', 'localcolor', 'localcolor_err', 'prompt']\n",
    "select_columns = ['magobs', 'magobs_err', 'c', 'c_err', 'x1','x1_err', 'localcolor', 'localcolor_err'] # , 'z'\n",
    "select_params  = list(params.keys())  # or subset you want\n",
    "\n",
    "data_stats = {}\n",
    "for col in select_columns:\n",
    "    # flatten over all objects in all examples\n",
    "    all_values = jnp.concatenate([data[col].ravel() for data in [data]])  # replace with training set list if needed\n",
    "    mu = jnp.mean(all_values)\n",
    "    sigma = jnp.std(all_values) + 1e-8\n",
    "    data_stats[col] = {'mu': mu, 'sigma': sigma}\n",
    "\n",
    "data_padded_normed = {}\n",
    "for col in select_columns:\n",
    "    x = data_padded[col]\n",
    "    mu = data_stats[col]['mu']\n",
    "    sigma = data_stats[col]['sigma']\n",
    "    x_norm = (x - mu) / sigma\n",
    "    x_norm = jnp.where(mask, x_norm, 0.0)\n",
    "    data_padded_normed[col] = x_norm\n",
    "\n",
    "data_padded = data_padded_normed\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_stats = {}\n",
    "for dic_key in select_params:\n",
    "    all_values = jnp.concatenate([params[dic_key].ravel() for params in [params]])  # replace with training set list\n",
    "    mu = jnp.mean(all_values)\n",
    "    sigma = jnp.std(all_values) + 1e-8\n",
    "    param_stats[dic_key] = {'mu': mu, 'sigma': sigma}\n",
    "\n",
    "normalized_priors = {}\n",
    "\n",
    "for name, prior in priors.items():\n",
    "    mu = param_stats[name]['mu']\n",
    "    sigma = param_stats[name]['sigma']\n",
    "\n",
    "    norm_range = (prior['range'] - mu) / sigma\n",
    "        \n",
    "    normalized_priors[name] = {\n",
    "        'range': norm_range,\n",
    "        'type': prior['type']  # type stays unchanged\n",
    "    }\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data   = nntr.normalize(data, data_stats)\n",
    "params = nntr.normalize(params, param_stats)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Autoregressive dataset construction\n",
    "# -----------------------------------------\n",
    "\n",
    "param_names = list(params.keys())\n",
    "K = len(param_names)  # total number of parameters\n",
    "\n",
    "# Generate false parameters directly from priors\n",
    "# priors['sigma_int']['type'] = 'positive-gaussian'\n",
    "# priors['sigma_int']['range'] = jnp.array([0.1, 0.4])\n",
    "\n",
    "false_params = gsim.scan_params(priors, N)\n",
    "\n",
    "false_params = jnp.stack(\n",
    "    [false_params[name] for name in param_names],\n",
    "    axis=1,   # (N, K)\n",
    ")\n",
    "\n",
    "# Extract mus and sigmas in the same order\n",
    "mus = jnp.array([param_stats[name]['mu'] for name in param_names])\n",
    "sigmas = jnp.array([param_stats[name]['sigma'] for name in param_names])\n",
    "\n",
    "# Normalize\n",
    "false_params = (false_params - mus) / sigmas\n",
    "\n",
    "# True params in array form\n",
    "true_params = jnp.stack([params[name] for name in param_names], axis=1)  # (N,K)\n",
    "\n",
    "# ----- Observational data concatenation (unchanged) -----\n",
    "data_names = list(data_padded.keys())\n",
    "n_cols = len(data_names)\n",
    "\n",
    "data_arrays = [data_padded[name] for name in data_names]\n",
    "data_stacked = jnp.stack(data_arrays, axis=-1)  # (N, M, n_cols)\n",
    "data_concat  = data_stacked.reshape(N, M * n_cols)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Global train / test split (shared across all groups)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "key, split_key = jax.random.split(key)\n",
    "\n",
    "indices = jnp.arange(N)\n",
    "perm = jax.random.permutation(split_key, indices)\n",
    "\n",
    "n_test = int(0.3 * N)\n",
    "test_idx  = perm[:n_test]\n",
    "train_idx = perm[n_test:]\n",
    "\n",
    "# Slice once\n",
    "data_train  = data_concat[train_idx]\n",
    "data_test   = data_concat[test_idx]\n",
    "\n",
    "param_true_train  = true_params[train_idx]\n",
    "param_true_test   = true_params[test_idx]\n",
    "\n",
    "false_params_train = false_params[train_idx]\n",
    "false_params_test  = false_params[test_idx]\n",
    "\n",
    "mask_train = mask[train_idx]\n",
    "mask_test  = mask[test_idx]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Build parameter slices per group (correct semantics)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "all_group_param_slices = []\n",
    "\n",
    "for g, group in enumerate(param_groups):\n",
    "\n",
    "    group_list = [group] if isinstance(group, str) else group\n",
    "    group_idx = jnp.array([param_names.index(name) for name in group_list])\n",
    "\n",
    "    prev_groups = [\n",
    "        p for i in range(g)\n",
    "        for p in (param_groups[i] if isinstance(param_groups[i], list) else [param_groups[i]])\n",
    "    ]\n",
    "    prev_idx = (\n",
    "        jnp.array([param_names.index(name) for name in prev_groups], dtype=int)\n",
    "        if prev_groups else jnp.array([], dtype=int)\n",
    "    )\n",
    "\n",
    "    visible_idx = (\n",
    "        jnp.concatenate([prev_idx, group_idx], axis=0)\n",
    "        if prev_idx.size > 0 else group_idx\n",
    "    )\n",
    "\n",
    "    # --------------------\n",
    "    # Labels\n",
    "    # --------------------\n",
    "    key, label_key1, label_key2 = jax.random.split(key,3)\n",
    "    labels_train = jax.random.uniform(label_key1, (train_idx.shape[0],)) > 0.5\n",
    "    labels_test  = jax.random.uniform(label_key2, (test_idx.shape[0],))  > 0.5\n",
    "\n",
    "    # --------------------\n",
    "    # Build params (ONLY flip current group)\n",
    "    # --------------------\n",
    "    params_train = jnp.array(param_true_train)\n",
    "    params_train = params_train.at[:, group_idx].set(\n",
    "        jnp.where(\n",
    "            labels_train[:, None],\n",
    "            param_true_train[:, group_idx],\n",
    "            false_params_train[:, group_idx],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    params_test = jnp.array(param_true_test)\n",
    "    params_test = params_test.at[:, group_idx].set(\n",
    "        jnp.where(\n",
    "            labels_test[:, None],\n",
    "            param_true_test[:, group_idx],\n",
    "            false_params_test[:, group_idx],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    chosen_train = params_train[:, visible_idx]\n",
    "    chosen_test  = params_test[:,  visible_idx]\n",
    "\n",
    "    all_group_param_slices.append({\n",
    "        \"chosen_train\": chosen_train,\n",
    "        \"chosen_test\":  chosen_test,\n",
    "        \"labels_train\": labels_train,\n",
    "        \"labels_test\":  labels_test,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the different network layers\n",
    "Nsize_e = 32 #32\n",
    "Nsize_p = 64 #64 \n",
    "Nsize_r = 128 #256 \n",
    "\n",
    "n_cols = len(data_names)\n",
    "print('# of columns :', n_cols)\n",
    "n_params = len(param_names)\n",
    "print('# of params :', n_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Prepare a list of models, one per group\n",
    "# ----------------------------\n",
    "\n",
    "models_per_group = []\n",
    "group_configs = []\n",
    "rng = nnx.Rngs(0)\n",
    "\n",
    "for g, group in enumerate(param_groups):\n",
    "    # Determine number of parameters visible for this group\n",
    "    prev_groups = [\n",
    "        p\n",
    "        for i in range(g)\n",
    "        for p in (param_groups[i] if isinstance(param_groups[i], list) else [param_groups[i]])\n",
    "    ]\n",
    "    group_list = [group] if isinstance(group, str) else group\n",
    "    visible_param_names = prev_groups + group_list\n",
    "    n_params_visible = len(visible_param_names)\n",
    "\n",
    "    print(\n",
    "        f\"Group {g}: visible parameters = {visible_param_names}, \"\n",
    "        f\"total = {n_params_visible}\"\n",
    "    )\n",
    "\n",
    "    # Create DeepSetClassifier for this group\n",
    "    model_g = nntr.DeepSetClassifier(\n",
    "        dropout_rate=0.1,\n",
    "        Nsize_p=Nsize_p,\n",
    "        Nsize_r=Nsize_r,\n",
    "        n_cols=n_cols,\n",
    "        n_params=n_params_visible,\n",
    "        rngs=rng,\n",
    "    )\n",
    "\n",
    "    models_per_group.append(model_g)\n",
    "\n",
    "    # ---- CONFIG CAPTURE ----\n",
    "    group_configs.append({\n",
    "        \"group_id\": g,\n",
    "        \"n_params_visible\": n_params_visible,\n",
    "        \"visible_param_names\": visible_param_names,\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "model_config = {\n",
    "    \"shared\": {\n",
    "        \"Nsize_p\": Nsize_p,\n",
    "        \"Nsize_r\": Nsize_r,\n",
    "        \"Nsize_e\": Nsize_e,\n",
    "        # \"n_cols\": n_cols,\n",
    "        \"columns\": select_columns,\n",
    "        \"param_groups\": param_groups,\n",
    "        \"global_param_names\": global_param_names,\n",
    "        \"priors\": priors,\n",
    "        \"param_stats\": param_stats,\n",
    "        \"data_stats\": data_stats\n",
    "    },\n",
    "    \"groups\": group_configs,\n",
    "}\n",
    "\n",
    "# Optional: visualize one model\n",
    "nnx.display(models_per_group[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping parameters\n",
    "n_batch = 100\n",
    "patience = 20 #40\n",
    "epochs = 1000\n",
    "\n",
    "metrics_histories = []\n",
    "\n",
    "# init_values = jnp.linspace(3e-4, 3e-4, len(all_group_param_slices))\n",
    "# patiences   = jnp.linspace(40, 20,  len(all_group_param_slices))\n",
    "# epochss = [1,1,1,1000]\n",
    "# init_values = [1e-3, 1e-3, 1e-3, 1e-3]\n",
    "\n",
    "for g, group_data in enumerate(all_group_param_slices):\n",
    "\n",
    "    print(f\"\\n=== Training model for group {g}: {param_groups[g]} ===\")\n",
    "\n",
    "    chosen_train = group_data[\"chosen_train\"]\n",
    "    chosen_test  = group_data[\"chosen_test\"]\n",
    "    labels_train = group_data[\"labels_train\"]\n",
    "    labels_test  = group_data[\"labels_test\"]\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Concatenate inputs (no parameter logic here)\n",
    "    # ------------------------------------------------\n",
    "    train_data = jnp.concatenate(\n",
    "        [data_train, mask_train, chosen_train], axis=-1\n",
    "    )\n",
    "    test_data = jnp.concatenate(\n",
    "        [data_test, mask_test, chosen_test], axis=-1\n",
    "    )\n",
    "\n",
    "    train_labels = labels_train.astype(jnp.int32)[:, None]\n",
    "    test_labels  = labels_test.astype(jnp.int32)[:, None]\n",
    "\n",
    "    train_data   = jax.device_put(train_data, cpu)\n",
    "    train_labels = jax.device_put(train_labels, cpu)\n",
    "    test_data    = jax.device_put(test_data, cpu)\n",
    "    test_labels  = jax.device_put(test_labels, cpu)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Optimiser\n",
    "    # ------------------------------------------------\n",
    "    learning_rate_schedule = optax.exponential_decay(\n",
    "        init_value=5e-4,\n",
    "        transition_steps=1000,\n",
    "        decay_rate=0.9,\n",
    "    )\n",
    "\n",
    "    optimizer = nnx.Optimizer(\n",
    "        models_per_group[g],\n",
    "        optax.adamw(learning_rate_schedule, 0.9),\n",
    "    )\n",
    "\n",
    "    batch_size = train_data.shape[0] // n_batch\n",
    "\n",
    "    model_g, metrics_history, key = nntr.train_loop(\n",
    "        model=models_per_group[g],\n",
    "        optimizer=optimizer,\n",
    "        train_data=train_data,\n",
    "        train_labels=train_labels,\n",
    "        test_data=test_data,\n",
    "        test_labels=test_labels,\n",
    "        key=key,\n",
    "        epochs=epochs, #epochs #epochss[g]\n",
    "        batch_size=batch_size,\n",
    "        patience=patience,\n",
    "        metrics_history={\n",
    "            'train_loss': [],\n",
    "            'train_accuracy': [],\n",
    "            'test_loss': [],\n",
    "            'test_accuracy': []\n",
    "        },\n",
    "        M=M,\n",
    "        N=int(N * 0.7),\n",
    "        cpu=cpu,\n",
    "        gpu=gpu,\n",
    "        group_id=g,\n",
    "        group_params=param_groups[g],\n",
    "        plot_flag=True,\n",
    "    )\n",
    "    \n",
    "    models_per_group[g] = model_g\n",
    "    metrics_histories.append(metrics_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set models to evaluation mode\n",
    "for model_g in models_per_group:\n",
    "    model_g.eval()  # disable dropout, etc.\n",
    "\n",
    "batch_size = 128\n",
    "metrics_per_group = []\n",
    "\n",
    "# Loop over groups\n",
    "for g, model_g in enumerate(models_per_group):\n",
    "\n",
    "    print(f\"\\n=== Evaluating model for group {g}: {param_groups[g]} ===\")\n",
    "\n",
    "    chosen_test  = all_group_param_slices[g][\"chosen_test\"]\n",
    "    labels_test  = all_group_param_slices[g][\"labels_test\"]\n",
    "\n",
    "    num_samples = labels_test.shape[0]\n",
    "\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "\n",
    "        xb = jnp.concatenate(\n",
    "            [\n",
    "                data_test[i:i + batch_size],\n",
    "                mask_test[i:i + batch_size],\n",
    "                chosen_test[i:i + batch_size],\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "\n",
    "        yb = labels_test[i:i + batch_size, None].astype(jnp.int32)\n",
    "\n",
    "        # Model predictions\n",
    "        logits = nntr.pred_step(model_g, xb)\n",
    "        all_logits.append(logits)\n",
    "        all_labels.append(yb)\n",
    "\n",
    "    # Merge batches\n",
    "    all_logits = jnp.concatenate(all_logits, axis=0)\n",
    "    all_labels = jnp.concatenate(all_labels, axis=0)\n",
    "\n",
    "    all_preds = (jsp.special.expit(all_logits) > 0.5).astype(jnp.int32)\n",
    "\n",
    "    # Confusion matrix components\n",
    "    TP = jnp.sum((all_preds == 1) & (all_labels == 1))\n",
    "    TN = jnp.sum((all_preds == 0) & (all_labels == 0))\n",
    "    FP = jnp.sum((all_preds == 1) & (all_labels == 0))\n",
    "    FN = jnp.sum((all_preds == 0) & (all_labels == 1))\n",
    "\n",
    "    accuracy    = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision   = TP / (TP + FP + 1e-8)\n",
    "    sensitivity = TP / (TP + FN + 1e-8)\n",
    "    specificity = TN / (TN + FP + 1e-8)\n",
    "\n",
    "    print(\n",
    "        f\"Group {g} ({param_groups[g]}): \"\n",
    "        f\"Accuracy={accuracy:.3f}, \"\n",
    "        f\"Precision={precision:.3f}, \"\n",
    "        f\"Sensitivity={sensitivity:.3f}, \"\n",
    "        f\"Specificity={specificity:.3f}\"\n",
    "    )\n",
    "\n",
    "    metrics_per_group.append({\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"sensitivity\": sensitivity,\n",
    "        \"specificity\": specificity,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_g in models_per_group:\n",
    "    model_g.eval()  # disable dropout, etc.\n",
    "\n",
    "# ----------------------------\n",
    "# Parameter info\n",
    "# ----------------------------\n",
    "param_names = list(params.keys())\n",
    "N_SIM_PARAMS = len(param_names)\n",
    "\n",
    "# ----------------------------\n",
    "# LAST GROUP: precomputed params + labels\n",
    "# ----------------------------\n",
    "chosen_test = all_group_param_slices[-1][\"chosen_test\"]\n",
    "labels_test = all_group_param_slices[-1][\"labels_test\"]\n",
    "\n",
    "# ----------------------------\n",
    "# Mask for \"true\" samples (label == 1)\n",
    "# ----------------------------\n",
    "mask_true = labels_test == 1\n",
    "N_sims = int(jnp.minimum(100, jnp.sum(mask_true)))\n",
    "\n",
    "# Get indices of true samples\n",
    "true_idx = jnp.nonzero(mask_true, size=N_sims, fill_value=0)[0]\n",
    "\n",
    "NDIM = len(global_param_names)\n",
    "\n",
    "# ----------------------------\n",
    "# Construct test inputs and full theta\n",
    "# ----------------------------\n",
    "# visible parameters for true samples\n",
    "theta_star = chosen_test[true_idx]\n",
    "\n",
    "# inputs excluding theta (data + mask)\n",
    "xy_test = jnp.concatenate(\n",
    "    [data_test[true_idx, :], mask_test[true_idx, :]],\n",
    "    axis=-1\n",
    ")\n",
    "\n",
    "alpha_grid = jnp.linspace(0, 1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 10\n",
    "\n",
    "# Convert theta_star to dict for unnormalisation\n",
    "theta_star_dict = {name: theta_star[index, i] for i, name in enumerate(global_param_names)}\n",
    "\n",
    "# Unnormalize\n",
    "theta_star_unnormed_dict = nntr.unnormalize(theta_star_dict, param_stats)\n",
    "\n",
    "# Convert back to array in the same order as global_param_names\n",
    "theta_star_unnormed = jnp.array([theta_star_unnormed_dict[name] for name in global_param_names])\n",
    "\n",
    "print(param_groups)\n",
    "print(theta_star_unnormed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names_list = []\n",
    "for g in param_groups:\n",
    "    if isinstance(g, str):\n",
    "        group_names_list.append([g])  # wrap single parameter in a list\n",
    "    else:\n",
    "        group_names_list.append(g)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single test sample (or batch) as input\n",
    "test_data = xy_test[index, :]  # single sample, shape (n_features,)\n",
    "\n",
    "# Initial position at the middle of priors\n",
    "theta_init = theta_star[index,:] #(BOUNDS[:, 0] + BOUNDS[:, 1]) / 2.0\n",
    "\n",
    "visible_indices, group_indices = nnte.preprocess_groups(param_groups, global_param_names)\n",
    "\n",
    "priors_inference = priors\n",
    "\n",
    "normalized_priors_inference = {}\n",
    "\n",
    "for name, prior in priors_inference.items():\n",
    "    mu = param_stats[name]['mu']\n",
    "    sigma = param_stats[name]['sigma']\n",
    "\n",
    "    norm_range = (prior['range'] - mu) / sigma\n",
    "    \n",
    "    normalized_priors_inference[name] = {\n",
    "        'range': norm_range,\n",
    "        'type': prior['type']  # type stays unchanged\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def log_post(theta):\n",
    "    # Use the new grouped log-prob function\n",
    "    return nnte.log_prob_fn_groups(\n",
    "        theta,\n",
    "        models_per_group,  # list of models per group\n",
    "        test_data,\n",
    "        normalized_priors_inference,\n",
    "        visible_indices,\n",
    "        group_indices,\n",
    "        group_names_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MCMC\n",
    "print(\"Launch MCMC ...\")\n",
    "key, post = nnte.sample_posterior(\n",
    "    log_post,\n",
    "    n_warmup=100,\n",
    "    n_samples=100,\n",
    "    init_position=theta_init,\n",
    "    rng_key=key\n",
    ")\n",
    "print(\"...finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_params = post.shape\n",
    "\n",
    "# Convert post from array to dict with column names\n",
    "post_dict = {name: post[:, i] for i, name in enumerate(global_param_names)}\n",
    "\n",
    "# Unnormalize each parameter\n",
    "post_unnormed_dict = nntr.unnormalize(post_dict, param_stats)\n",
    "\n",
    "# Convert back to array for plotting\n",
    "post_unnormed = jnp.stack([post_unnormed_dict[name] for name in global_param_names], axis=1)\n",
    "\n",
    "# Now post_unnormed[:, i] contains the real-scale parameters\n",
    "x = post_unnormed[:, 0]\n",
    "\n",
    "ranges_from_priors = [\n",
    "    (float(priors[name]['range'][0])-(float(priors[name]['range'][1])-float(priors[name]['range'][0]))/2, float(priors[name]['range'][1])+(float(priors[name]['range'][1])-float(priors[name]['range'][0]))/2)\n",
    "    for name in global_param_names\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = corner.corner(\n",
    "    np.array(post_unnormed),\n",
    "    labels=global_param_names,\n",
    "    range=ranges_from_priors,\n",
    "    quantiles=[0.16, 0.5, 0.84],      # 1D marginal: 1σ\n",
    "    levels=[0.393469, 0.864665],     # 2D contours: 1σ, 2σ\n",
    "    show_titles=True,\n",
    "    title_fmt=\".4f\",\n",
    "    bins=30,\n",
    "    smooth=1.0,\n",
    "    color=\"#1F487E\",\n",
    "    truths=theta_star_unnormed[:],\n",
    "    truth_color=\"#A31621\", \n",
    "    truth_alpha=0.8\n",
    ")\n",
    "\n",
    "plt.savefig(\"./corner.png\",dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save NN to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model for future use\n",
    "save_path = '../data/NNs/nn_model_priors_M1000_cosmo_err_z_small_sample' #nn_model_priors_M1000_cosmo_err_z_smaller_dropout\n",
    "nntr.save_autoregressive_nn(models_per_group,save_path, model_config)\n",
    "print('NNs saved to ' + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi",
   "language": "python",
   "name": "sbi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
